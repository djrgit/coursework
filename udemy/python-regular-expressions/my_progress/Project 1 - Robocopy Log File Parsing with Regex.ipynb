{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'rocopylog.txt'\n",
    "file2 = 'rocopylog_invalid_source.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open one of the files above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern to Identify Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN_SOURCE_DESTN = re.compile(r'\\s+(?P<type>Source|Dest) : (?P<dir>C.*\\b)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_header(lines, pattern):\n",
    "    header_dict_regex = {'type': [], 'dir': []}\n",
    "    for line in lines:\n",
    "        source_destn_data_from_regex = re.finditer(pattern, line)\n",
    "        for match in source_destn_data_from_regex:\n",
    "            header_dict_regex['type'].append(match.group('type'))\n",
    "            header_dict_regex['dir'].append(match.group('dir'))\n",
    "    return header_dict_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = identify_header(open_file(file1), PATTERN_SOURCE_DESTN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type ['Source', 'Dest']\n",
      "dir ['C:\\\\RegularExpressionsWithDotNet\\\\robocopytest\\\\source\\\\தமிழ்\\\\हिन्दी\\\\English', 'C:\\\\RegularExpressionsWithDotNet\\\\robocopytest\\\\destn']\n"
     ]
    }
   ],
   "source": [
    "for key, val in headers.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern to Capture Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN_ERROR_MSG = re.compile(r'(?P<ts>\\b\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}) ERROR '\\\n",
    "                               r'(?P<error>.*\\b)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_error_msg(lines, pattern):\n",
    "    error_dict_regex = {'ts': [], 'error': []}\n",
    "    for line in lines:\n",
    "        error_msg_data_from_regex = re.finditer(pattern, line)\n",
    "        for match in error_msg_data_from_regex:\n",
    "            error_dict_regex['ts'].append(match.group('ts'))\n",
    "            error_dict_regex['error'].append(match.group('error'))\n",
    "    return error_dict_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = capture_error_msg(open_file(file2), PATTERN_ERROR_MSG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts ['2016/05/28 19:24:30']\n",
      "error ['2 (0x00000002) Accessing Source Directory C:\\\\RegularExpressionsWithDotNet\\\\robocopytest\\\\source2']\n"
     ]
    }
   ],
   "source": [
    "for key, val in errors.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern to Capture Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN_METRICS_TBL = re.compile(r'\\s+(?P<type>Dirs|Files|Bytes) :\\s+'\\\n",
    "                                 r'(?P<total>\\d+)\\s+'\\\n",
    "                                 r'(?P<copied>\\d+)\\s+'\\\n",
    "                                 r'(?P<skipped>\\d+)\\s+'\\\n",
    "                                 r'(?P<mismatch>\\d+)\\s+'\\\n",
    "                                 r'(?P<failed>\\d+)\\s+'\\\n",
    "                                 r'(?P<extras>\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_metrics_tbl(lines, pattern):\n",
    "    metrics_dict_regex = {'type': [], 'total': [], 'copied': [], 'skipped': [], \\\n",
    "                          'mismatch': [], 'failed': [], 'extras': []}\n",
    "    for line in lines:\n",
    "        metrics_data_from_regex = re.finditer(pattern, line)\n",
    "        for match in metrics_data_from_regex:\n",
    "            metrics_dict_regex['type'].append(match.group('type'))\n",
    "            metrics_dict_regex['total'].append(match.group('total'))\n",
    "            metrics_dict_regex['copied'].append(match.group('copied'))\n",
    "            metrics_dict_regex['skipped'].append(match.group('skipped'))\n",
    "            metrics_dict_regex['mismatch'].append(match.group('mismatch'))\n",
    "            metrics_dict_regex['failed'].append(match.group('failed'))\n",
    "            metrics_dict_regex['extras'].append(match.group('extras'))\n",
    "    return metrics_dict_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = capture_metrics_tbl(open_file(file1), PATTERN_METRICS_TBL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type ['Dirs', 'Files', 'Bytes']\n",
      "total ['7', '29', '133567']\n",
      "copied ['6', '29', '133567']\n",
      "skipped ['1', '0', '0']\n",
      "mismatch ['0', '0', '0']\n",
      "failed ['0', '0', '0']\n",
      "extras ['0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "for key, val in metrics.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert dictionaries to json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": [\"Source\", \"Dest\"], \"dir\": [\"C:\\\\RegularExpressionsWithDotNet\\\\robocopytest\\\\source\\\\\\u0ba4\\u0bae\\u0bbf\\u0bb4\\u0bcd\\\\\\u0939\\u093f\\u0928\\u094d\\u0926\\u0940\\\\English\", \"C:\\\\RegularExpressionsWithDotNet\\\\robocopytest\\\\destn\"]}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ts\": [\"2016/05/28 19:24:30\"], \"error\": [\"2 (0x00000002) Accessing Source Directory C:\\\\RegularExpressionsWithDotNet\\\\robocopytest\\\\source2\"]}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": [\"Dirs\", \"Files\", \"Bytes\"], \"total\": [\"7\", \"29\", \"133567\"], \"copied\": [\"6\", \"29\", \"133567\"], \"skipped\": [\"1\", \"0\", \"0\"], \"mismatch\": [\"0\", \"0\", \"0\"], \"failed\": [\"0\", \"0\", \"0\"], \"extras\": [\"0\", \"0\", \"0\"]}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('headers.json','w', encoding='utf-8') as wr:\n",
    "        json.dump(headers, wr, ensure_ascii=False, indent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('errors.json','w', encoding='utf-8') as wr:\n",
    "        json.dump(errors, wr, ensure_ascii=False, indent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('metrics.json','w', encoding='utf-8') as wr:\n",
    "        json.dump(metrics, wr, ensure_ascii=False, indent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
