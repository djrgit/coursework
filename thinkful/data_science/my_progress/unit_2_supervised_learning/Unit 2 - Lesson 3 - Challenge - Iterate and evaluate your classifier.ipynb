{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - Iterate and Evaluate Your Classifier\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source of sentiment words data (gathered November 2018):\n",
    " - [http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Citations:\n",
    "   - Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "       Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "       Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "       Washington, USA, \n",
    "   - Bing Liu, Minqing Hu and Junsheng Cheng. \"Opinion Observer: Analyzing \n",
    "       and Comparing Opinions on the Web.\" Proceedings of the 14th \n",
    "       International World Wide Web conference (WWW-2005), May 10-14, \n",
    "       2005, Chiba, Japan.\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules and enable the display of plots in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore harmless seaborn warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the positive and negative sentiment word datasets into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_words = 'https://raw.githubusercontent.com/djrgit/coursework/master/thinkful/data_science/my_progress/unit_2_supervised_learning/positive_words.txt'\n",
    "neg_words = 'https://raw.githubusercontent.com/djrgit/coursework/master/thinkful/data_science/my_progress/unit_2_supervised_learning/negative_words.txt'\n",
    "\n",
    "pos = pd.read_csv(pos_words)\n",
    "neg = pd.read_csv(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_words = [pos, neg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variable names for website review datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_amazon = 'https://raw.githubusercontent.com/djrgit/coursework/master/thinkful/data_science/my_progress/unit_2_supervised_learning/amazon_cells_labelled.txt'\n",
    "label_imdb = 'https://raw.githubusercontent.com/djrgit/coursework/master/thinkful/data_science/my_progress/unit_2_supervised_learning/imdb_labelled.txt'\n",
    "label_yelp = 'https://raw.githubusercontent.com/djrgit/coursework/master/thinkful/data_science/my_progress/unit_2_supervised_learning/yelp_labelled.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [label_amazon, label_imdb, label_yelp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to build an initial DataFrame from website reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_site_dataframe(website_reviews):\n",
    "    wr = requests.get(website_reviews).content\n",
    "    df = pd.read_table(io.StringIO(wr.decode('utf-8')), \n",
    "                       delimiter='\\t\\d\\n', header=None, \n",
    "                       engine='python')\n",
    "    df = df[0].str.split(pat='\\t', n=-1, expand=True)\n",
    "    df = df.rename(columns={0: 'review', 1: 'sentiment'})\n",
    "    df = df[['sentiment', 'review']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to add positive and negative sentiment words to the website review DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sent_words_to_df(df, sent_words):\n",
    "    for sent in sent_words:\n",
    "        for word in sent.iloc[:,0]:\n",
    "            df[str(word)] = df['review'].str.lower().str.contains(str(word), regex=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to run a Bernoulli Naive Bayes classifier on a prepped DataFrame with additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Test your model with different holdout groups.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bernoulli(prepped_df, site):\n",
    "    \n",
    "    # Include all additional word features in the data\n",
    "    data = prepped_df.iloc[:,2:]\n",
    "    \n",
    "    target = prepped_df['sentiment']\n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    bnb.fit(data, target)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "    y_pred = bnb.predict(data)\n",
    "\n",
    "    # Display our results.\n",
    "    print(site[site.rfind('/')+1:site.rfind('_')])\n",
    "    print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], \n",
    "                                                                             (target != y_pred).sum()))\n",
    "    \n",
    "    # Use train_test_split to create the necessary training and test groups\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=101)\n",
    "    fit_train = bnb.fit(X_train, y_train)\n",
    "    print('With 20% Holdout: ' + str(fit_train.score(X_test, y_test)))\n",
    "    predictions = bnb.predict(X_test)\n",
    "    print('Confusion matrix: ')\n",
    "    print(confusion_matrix(predictions, y_test))\n",
    "    print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
    "\n",
    "    print('Cross validation scores (5 folds): ')\n",
    "    print(np.array(cross_val_score(bnb, data, target, cv=5)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the above functions in a data pipeline for all website review datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_cells\n",
      "Number of mislabeled points out of a total 1000 points : 137\n",
      "With 20% Holdout: 0.66\n",
      "Confusion matrix: \n",
      "[[46  1]\n",
      " [67 86]]\n",
      "Testing on Sample: 0.863\n",
      "Cross validation scores (5 folds): \n",
      "[0.77  0.82  0.785 0.745 0.785]\n",
      "\n",
      "\n",
      "imdb\n",
      "Number of mislabeled points out of a total 1000 points : 123\n",
      "With 20% Holdout: 0.735\n",
      "Confusion matrix: \n",
      "[[84 38]\n",
      " [15 63]]\n",
      "Testing on Sample: 0.877\n",
      "Cross validation scores (5 folds): \n",
      "[0.75  0.76  0.805 0.74  0.78 ]\n",
      "\n",
      "\n",
      "yelp\n",
      "Number of mislabeled points out of a total 1000 points : 144\n",
      "With 20% Holdout: 0.725\n",
      "Confusion matrix: \n",
      "[[56  9]\n",
      " [46 89]]\n",
      "Testing on Sample: 0.856\n",
      "Cross validation scores (5 folds): \n",
      "[0.795 0.775 0.75  0.8   0.805]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for site in sites:\n",
    "    run_bernoulli(add_sent_words_to_df(build_site_dataframe(site), sent_words), site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to add positive and negative sentiment word counts to the website review DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sent_words_to_df_r1(df, sent_words):\n",
    "    \n",
    "    df[sent_words[0].columns[0]] = 0\n",
    "    df[sent_words[1].columns[0]] = 0\n",
    "    df['pos_minus_neg'] = 0\n",
    "    df['pos_minus_neg_gt_0'] = 0\n",
    "    \n",
    "    for sent in sent_words:\n",
    "        for word in sent.iloc[:,0]:\n",
    "            df[str(word)] = df['review'].str.lower().str.contains(str(word), regex=False)\n",
    "            df[sent.columns[0]] += df['review'].map(lambda r: r.lower().count(str(word)))\n",
    "            \n",
    "    df['pos_minus_neg'] = df[sent_words[0].columns[0]] - df[sent_words[1].columns[0]]\n",
    "    df['pos_minus_neg_gt_0'] = df['pos_minus_neg'] > 0\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Model 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Bernoulli model training data on whether sentiment is more positive or negative/neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bernoulli_r1(prepped_df, site):\n",
    "    \n",
    "    # Base data on a single feature of whether reviews contain more positive or negative/neutral features\n",
    "    data = prepped_df.iloc[:,5:6]\n",
    "    \n",
    "    target = prepped_df['sentiment']\n",
    "    \n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    bnb.fit(data, target)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "    y_pred = bnb.predict(data)\n",
    "\n",
    "    # Display our results.\n",
    "    print(site[site.rfind('/')+1:site.rfind('_')])\n",
    "    print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], \n",
    "                                                                             (target != y_pred).sum()))\n",
    "    \n",
    "    # Use train_test_split to create the necessary training and test groups\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=101)\n",
    "    fit_train = bnb.fit(X_train, y_train)\n",
    "    print('With 20% Holdout: ' + str(fit_train.score(X_test, y_test)))\n",
    "    predictions = bnb.predict(X_test)\n",
    "    print('Confusion matrix: ')\n",
    "    print(confusion_matrix(predictions, y_test))\n",
    "    print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
    "\n",
    "    print('Cross validation scores (5 folds): ')\n",
    "    print(np.array(cross_val_score(bnb, data, target, cv=5)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_cells\n",
      "Number of mislabeled points out of a total 1000 points : 212\n",
      "With 20% Holdout: 0.775\n",
      "Confusion matrix: \n",
      "[[90 22]\n",
      " [23 65]]\n",
      "Testing on Sample: 0.788\n",
      "Cross validation scores (5 folds): \n",
      "[0.78  0.825 0.775 0.785 0.775]\n",
      "\n",
      "\n",
      "imdb\n",
      "Number of mislabeled points out of a total 1000 points : 285\n",
      "With 20% Holdout: 0.685\n",
      "Confusion matrix: \n",
      "[[87 51]\n",
      " [12 50]]\n",
      "Testing on Sample: 0.715\n",
      "Cross validation scores (5 folds): \n",
      "[0.69  0.72  0.71  0.7   0.755]\n",
      "\n",
      "\n",
      "yelp\n",
      "Number of mislabeled points out of a total 1000 points : 277\n",
      "With 20% Holdout: 0.745\n",
      "Confusion matrix: \n",
      "[[80 29]\n",
      " [22 69]]\n",
      "Testing on Sample: 0.723\n",
      "Cross validation scores (5 folds): \n",
      "[0.745 0.75  0.69  0.715 0.715]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for site in sites:\n",
    "    run_bernoulli_r1(add_sent_words_to_df_r1(build_site_dataframe(site), sent_words), site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Model 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine 'Revised Model 1' with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bernoulli_r2(prepped_df, site):\n",
    "    \n",
    "    # Base data on combined features from 'Revised Model 1' with the original model\n",
    "    data = prepped_df.iloc[:,5:]\n",
    "    \n",
    "    target = prepped_df['sentiment']\n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    bnb = BernoulliNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    bnb.fit(data, target)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "    y_pred = bnb.predict(data)\n",
    "\n",
    "    # Display our results.\n",
    "    print(site[site.rfind('/')+1:site.rfind('_')])\n",
    "    print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], \n",
    "                                                                             (target != y_pred).sum()))\n",
    "    \n",
    "    # Use train_test_split to create the necessary training and test groups\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=101)\n",
    "    fit_train = bnb.fit(X_train, y_train)\n",
    "    print('With 20% Holdout: ' + str(fit_train.score(X_test, y_test)))\n",
    "    predictions = bnb.predict(X_test)\n",
    "    print('Confusion matrix: ')\n",
    "    print(confusion_matrix(predictions, y_test))\n",
    "    print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
    "\n",
    "    print('Cross validation scores (5 folds): ')\n",
    "    print(np.array(cross_val_score(bnb, data, target, cv=5)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_cells\n",
      "Number of mislabeled points out of a total 1000 points : 157\n",
      "With 20% Holdout: 0.795\n",
      "Confusion matrix: \n",
      "[[85 13]\n",
      " [28 74]]\n",
      "Testing on Sample: 0.843\n",
      "Cross validation scores (5 folds): \n",
      "[0.805 0.85  0.77  0.795 0.8  ]\n",
      "\n",
      "\n",
      "imdb\n",
      "Number of mislabeled points out of a total 1000 points : 139\n",
      "With 20% Holdout: 0.755\n",
      "Confusion matrix: \n",
      "[[85 35]\n",
      " [14 66]]\n",
      "Testing on Sample: 0.861\n",
      "Cross validation scores (5 folds): \n",
      "[0.74 0.78 0.82 0.77 0.8 ]\n",
      "\n",
      "\n",
      "yelp\n",
      "Number of mislabeled points out of a total 1000 points : 179\n",
      "With 20% Holdout: 0.795\n",
      "Confusion matrix: \n",
      "[[78 17]\n",
      " [24 81]]\n",
      "Testing on Sample: 0.821\n",
      "Cross validation scores (5 folds): \n",
      "[0.765 0.77  0.76  0.78  0.77 ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for site in sites:\n",
    "    run_bernoulli_r2(add_sent_words_to_df_r1(build_site_dataframe(site), sent_words), site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sent_words_to_df_r3(df, sent_words):\n",
    "    \n",
    "    df[sent_words[0].columns[0]] = 0\n",
    "    df[sent_words[1].columns[0]] = 0\n",
    "    df['pos_minus_neg'] = 0\n",
    "    df['pos_minus_neg_gt_0'] = 0\n",
    "    \n",
    "    for sent in sent_words:\n",
    "        for word in sent.iloc[:,0]:\n",
    "            df[str(word)] = df['review'].map(lambda r: r.lower().count(str(word)))\n",
    "            df[sent.columns[0]] += df['review'].map(lambda r: r.lower().count(str(word)))\n",
    "            \n",
    "    df['pos_minus_neg'] = df[sent_words[0].columns[0]] - df[sent_words[1].columns[0]]\n",
    "    df['pos_minus_neg_gt_0'] = df['pos_minus_neg'] > 0\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to run a Multinomial Naive Bayes classifier on a prepped DataFrame with additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multinomial(prepped_df, site):\n",
    "    \n",
    "    # Include all additional word features in the data\n",
    "    data = prepped_df.iloc[:,6:]\n",
    "    \n",
    "    target = prepped_df['sentiment']\n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    mnb = MultinomialNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    mnb.fit(data, target)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "    y_pred = mnb.predict(data)\n",
    "\n",
    "    # Display our results.\n",
    "    print(site[site.rfind('/')+1:site.rfind('_')])\n",
    "    print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], \n",
    "                                                                             (target != y_pred).sum()))\n",
    "    \n",
    "    # Use train_test_split to create the necessary training and test groups\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=101)\n",
    "    fit_train = mnb.fit(X_train, y_train)\n",
    "    print('With 20% Holdout: ' + str(fit_train.score(X_test, y_test)))\n",
    "    predictions = mnb.predict(X_test)\n",
    "    print('Confusion matrix: ')\n",
    "    print(confusion_matrix(predictions, y_test))\n",
    "    print('Testing on Sample: ' + str(mnb.fit(data, target).score(data, target)))\n",
    "\n",
    "    print('Cross validation scores (5 folds): ')\n",
    "    print(np.array(cross_val_score(mnb, data, target, cv=5)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_cells\n",
      "Number of mislabeled points out of a total 1000 points : 132\n",
      "With 20% Holdout: 0.68\n",
      "Confusion matrix: \n",
      "[[55  6]\n",
      " [58 81]]\n",
      "Testing on Sample: 0.868\n",
      "Cross validation scores (5 folds): \n",
      "[0.745 0.81  0.78  0.74  0.79 ]\n",
      "\n",
      "\n",
      "imdb\n",
      "Number of mislabeled points out of a total 1000 points : 121\n",
      "With 20% Holdout: 0.76\n",
      "Confusion matrix: \n",
      "[[83 32]\n",
      " [16 69]]\n",
      "Testing on Sample: 0.879\n",
      "Cross validation scores (5 folds): \n",
      "[0.765 0.755 0.825 0.74  0.785]\n",
      "\n",
      "\n",
      "yelp\n",
      "Number of mislabeled points out of a total 1000 points : 145\n",
      "With 20% Holdout: 0.73\n",
      "Confusion matrix: \n",
      "[[59 11]\n",
      " [43 87]]\n",
      "Testing on Sample: 0.855\n",
      "Cross validation scores (5 folds): \n",
      "[0.785 0.755 0.75  0.785 0.785]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for site in sites:\n",
    "    run_multinomial(add_sent_words_to_df_r3(build_site_dataframe(site), sent_words), site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised Model 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sent_words_to_df_r4(df, sent_words):\n",
    "    \n",
    "    df['pos_minus_neg'] = 0\n",
    "    df[sent_words[0].columns[0]] = 0\n",
    "    df[sent_words[1].columns[0]] = 0\n",
    "    df['pos_minus_neg_gt_0'] = 0\n",
    "    \n",
    "    for sent in sent_words:\n",
    "        for word in sent.iloc[:,0]:\n",
    "            df[str(word)] = df['review'].map(lambda r: r.lower().count(str(word)))\n",
    "            df[sent.columns[0]] += df['review'].map(lambda r: r.lower().count(str(word)))\n",
    "            \n",
    "    df['pos_minus_neg'] = df[sent_words[0].columns[0]] - df[sent_words[1].columns[0]]\n",
    "    df['pos_minus_neg_gt_0'] = df['pos_minus_neg'] > 0\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include more features in the Multinomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multinomial_r4(prepped_df, site):\n",
    "    \n",
    "    # Include all numerical feature columns where ALL observations are not negative\n",
    "    data = prepped_df.iloc[:,3:]\n",
    "    \n",
    "    target = prepped_df['sentiment']\n",
    "    \n",
    "\n",
    "    # Instantiate our model and store it in a new variable.\n",
    "    mnb = MultinomialNB()\n",
    "\n",
    "    # Fit our model to the data.\n",
    "    mnb.fit(data, target)\n",
    "\n",
    "    # Classify, storing the result in a new variable.\n",
    "    y_pred = mnb.predict(data)\n",
    "\n",
    "    # Display our results.\n",
    "    print(site[site.rfind('/')+1:site.rfind('_')])\n",
    "    print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0], \n",
    "                                                                             (target != y_pred).sum()))\n",
    "    \n",
    "    # Use train_test_split to create the necessary training and test groups\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=101)\n",
    "    fit_train = mnb.fit(X_train, y_train)\n",
    "    print('With 20% Holdout: ' + str(fit_train.score(X_test, y_test)))\n",
    "    predictions = mnb.predict(X_test)\n",
    "    print('Confusion matrix: ')\n",
    "    print(confusion_matrix(predictions, y_test))\n",
    "    print('Testing on Sample: ' + str(mnb.fit(data, target).score(data, target)))\n",
    "\n",
    "    print('Cross validation scores (5 folds): ')\n",
    "    print(np.array(cross_val_score(mnb, data, target, cv=5)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_cells\n",
      "Number of mislabeled points out of a total 1000 points : 153\n",
      "With 20% Holdout: 0.745\n",
      "Confusion matrix: \n",
      "[[68  6]\n",
      " [45 81]]\n",
      "Testing on Sample: 0.847\n",
      "Cross validation scores (5 folds): \n",
      "[0.8   0.855 0.795 0.8   0.81 ]\n",
      "\n",
      "\n",
      "imdb\n",
      "Number of mislabeled points out of a total 1000 points : 152\n",
      "With 20% Holdout: 0.775\n",
      "Confusion matrix: \n",
      "[[82 28]\n",
      " [17 73]]\n",
      "Testing on Sample: 0.848\n",
      "Cross validation scores (5 folds): \n",
      "[0.78  0.775 0.865 0.79  0.795]\n",
      "\n",
      "\n",
      "yelp\n",
      "Number of mislabeled points out of a total 1000 points : 181\n",
      "With 20% Holdout: 0.77\n",
      "Confusion matrix: \n",
      "[[65  9]\n",
      " [37 89]]\n",
      "Testing on Sample: 0.819\n",
      "Cross validation scores (5 folds): \n",
      "[0.8   0.8   0.77  0.78  0.775]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for site in sites:\n",
    "    run_multinomial_r4(add_sent_words_to_df_r4(build_site_dataframe(site), sent_words), site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's time to revisit your classifier from the previous assignment. Using the evaluation techniques we've covered here, look at your classifier's performance in more detail. Then go back and iterate by engineering new features, removing poor features, or tuning parameters. Repeat this process until you have five different versions of your classifier. Once you've iterated, answer these questions to compare the performance of each:\n",
    "\n",
    "   - Do any of your classifiers seem to overfit?\n",
    "    >- _None of these models appear to exhibit overfitting, but an exploration of the error curves in a test set vs a cross-validation set and the training set would provide deeper insight._\n",
    "    \n",
    "   - Which seem to perform the best? Why?\n",
    "    >- _The **original model (Bernoulli)** and **Revised Model 3 (Multinomial)** both seem to perform the best (see sample accuracy values below).  All features in these models are based on the raw data itself, either by recording the presence of individual features or by recording their counts._\n",
    "        - Amazon, IMDB, Yelp  - Sample Accuracies\n",
    "        - 0.863, 0.877, 0.856 - Original **\n",
    "        - 0.788, 0.715, 0.723 - Rev 1\n",
    "        - 0.843, 0.861, 0.821 - Rev 2\n",
    "        - 0.868, 0.879, 0.855 - Rev 3 **\n",
    "        - 0.847, 0.848, 0.819 - Rev 4\n",
    "\n",
    "   \n",
    "   - Which features seemed to be most impactful to performance?\n",
    "    >- _Multinomial models seem to carry more information (quantity) about the data in each feature, as opposed to only boolean values._\n",
    "\n",
    "### Write up your iterations and answers to the above questions in a few pages. Submit a link below and go over it with your mentor to see if they have any other ideas on how you could improve your classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
